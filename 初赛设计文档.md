## 1. 5芯原杯初赛－设计文档

团队名称：自动化摸鱼队
团队成员：舒星研（队长）、李东昊、张含

## 2. 成果速览：

![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-01.jpg?height=1923&width=1698&top_left_y=784&top_left_x=183)

## 3. 一、方案介绍

－算法部分：本项目提出了一套轻量化、可嵌入的语音检测与说话人识别算法方案，涵盖从数据预处理、特征提取、模型设计、训练量化到部署后的后处理策略。该方案专为低资源嵌入式平台（RAM $<256 \mathrm{~KB}$ ）设计，结合MFCC声学特征、轻量神经网络和整数量化策略，兼顾准确性、计算效率和资源开销。整体算法分为两阶段处理：

- 第一阶段为语音活动检测（VAD），用于过滤非人声段，减少后续模型计算负担；
- 第二阶段为说话人识别（Speaker Identification），对检测到的人声片段进行分类。

所有模型在训练后均采用TFLite全整数量化，最终部署于QEMU仿真平台上进行验证，实现了端到端的嵌入式推理能力。
－嵌入式部分：使用环形缓冲区实现安全且高效的PCM数据内存管理，使用Event System实现task之间的通信，使用NMSIS－DSP库提供的MFCC函数实现MFCC运算，使用NMSIS－NN库提供的API实现卷积和池化操作，最终在QEMU上成功实现人声检测和说话人识别，算法更新频率为 1 秒。

## 4. 二、算法设计

![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-02.jpg?height=1275&width=797&top_left_y=1353&top_left_x=634)

## 5. 1．语音活动检测算法

- 目标：判断音频片段是否为人声片段
- 输入：1秒的音频片段（8000 采样率）
- 输出：是人声片段／不是人声片段
- 应用场景：部署到MCU上，作为后续说话人识别算法的前一级算法；兼顾准确性、存储占用和实时性能


## 6. 1 数据预处理

## 7. 1．1．1 数据集概况：

| 类别 | 描述 |
| :--- | :--- |
| voice | 纯人声 |
| blank | 无语音 but 含敲击、摩擦等杂音 |
| env | 环境背景音 |
| music | 纯音乐片段 |

- 采样率： 8 kHz
- 原始数据总时长：67．9分钟


## 8. 1．1．2 划分数据集并平衡类别间的数据比例

- 划分数据集：按 0．8：0．12：0．08 比例将数据集划分为训练集、验证集、测试集；
- 平衡比例：在每个集合内部将四类音频按照时长比例 voice：env：music：blank＝3：1：1：1进行最大取出，确保人声和噪声的数据比例为 $1: 1$


## 9. 1．1．3 数据切分与数据增强

－归一化：将所有PCM音频数据切分成时长为 1 秒（ 8000 个点）的语音片段，并逐片段进行归一化，归一化公式为

$$
\begin{aligned}
& x=\left\{x_{1}, x_{2}, \ldots, x_{n}\right\}, \quad x_{i} \in \mathbb{R} \\
& x_{i}^{\text {norm }}=\frac{x_{i}}{\max _{j}\left|x_{j}\right|}, \quad \forall i=1, \ldots, n
\end{aligned}
$$

- 数据增强：两两混音扩增
- 组合方式：4类声音两两排列组合（ $\mathrm{A}_{4}{ }^{2}$ ），共 12 种对。
- 多信噪比混音：每对片段进行 $-3 \mathrm{~dB} \quad 0 \mathrm{~dB} \quad 5 \mathrm{~dB} \quad 10 \mathrm{~dB} \quad 20 \mathrm{~dB}$ 五种不同信噪比的混音
- 总时长：原始时长 $\times(12 \times 5+1)$


## 10. 1．1．4 教师模型自动标注

## 11. －教师模型信息：

- 模型：Silero VAD（公开预训练、 32 ms 分辨率）。
- 输出：每 32 ms 一概率 p＿silero $\in[0,1]$
- 自动标注：每个语音片段（ 1 秒）会得到一个长度为 31 的概率序列，对其取平均，若大于 0.5 则将该片段标注为有语音，若小于 0.5 则将该片段标注为无语音。


## 12. 1．1．5 MFCC 特征提取

－切分：将每秒的语音片段（ 8000 个点）以每段 256 个点切分成 31 帧（尾部未满一个帧长的 64 个点被丢弃）
－MFCC：我们选用13维的MFCC以适配嵌入式对低资源占用的要求。对每帧进行MFCC，并取出第一维DC直流分量，得到每秒 $31 \times 12$ 的MFCC特征序列。

## 13. 2 模型架构与训练

## 14. 1．2．1 模型架构设计

- 为达到低功耗、低资源占用的目的，选择单卷积层＋单全连接层的网络架构，卷积核为 $3 \times 3$ 。
- 模型具体信息如下：
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-04.jpg?height=1025&width=255&top_left_y=1692&top_left_x=945)
－总参数量： $770(2.80 \mathrm{~KB})$


## 15. 1．2．2 训练与量化

－训练策略：

| 项目 | 配置 |
| :--- | :--- |
| 优化器 | Adam |
| 损失函数 | Categorical Crossentropy |
| 指标 | Accuracy |
| Epoch 数 | 20 |
| 批大小 | 32 |
| 验证集比例 | 按训练集拆分传入 |

- 使用交叉熵损失函数训练输出为 one－hot 编码的二分类；
- 在训练集上进行拟合，并在验证集上监控性能，防止过拟合；


## 16. －模型量化：

－为部署至嵌入式设备，训练完成后进行 全整数量化（Full Integer Quantization）：

| 项目 | 配置 |
| :--- | :--- |
| 输入类型 | int8 |
| 输出类型 | int8 |
| 支持算子 | TFLITE＿BUILTINS＿INT8 |
| 校准样本 | 前 100 个训练样本 |
| 代表数据集 | 采样前 100 个样本，使用 float32 格式参与量化校准 |

－通过 representative＿dataset 提供样本校准数据，生成低精度模型，适用于资源受限环境

## 17. 2．说话人识别算法

- 目标：判断语音片段是XiaoXin、XiaoYuan还是Others
- 输入：1秒的语音片段（8000 采样率）
- 输出：是哪个人
- 应用场景：部署到MCU上，作为语音活动检测算法的后一级算法，对说话人进行识别


## 18. 1 数据预处理

2．1．1 数据集概况：

| 类别 | 描述 |
| :--- | :--- |
| speakers＿board | 板载麦克风录制的说话人语音数据，包含ID1－ID5、XiaoXin 与 XiaoYuan |
| speakers＿pcphone | 手机或电脑麦克风录制的说话人语音数据，包含ID1－ID5、XiaoXin、 XiaoYuan、other＿speaker |
| noise／cat | 猫叫声 |
| noise／dog | 狗叫声 |
| noise／environment＿soun d | 环境声音 |
| noise／music | 纯音乐片段 |
| noise／nature＿sound | 自然声音（风雨水声等） |
| noise／noise | 杂类噪声 |

- 采样率： 8 kHz
- 原始数据总时长：462．7分钟


## 19. 2．1．2 平衡类别间比例

## 20. －说话人：

- 合并board和pc：将board和pc收集的相同人的声音合并在一起
- 合并others：将ID1－5和other＿speaker全部合并在一起
- 平衡比例：将XiaoXin、XiaoYuan、Others的比例平衡为 $1: 1: 1$
- 噪声：
- 平衡比例：使六类噪声的样本量保持一致
- 合并noise：将平衡比例后的六类噪声合并在一起


## 21. 2．1．3 数据切分与数据增强

－归一化：将所有PCM音频数据切分成时长为 1 秒（ 8000 个点）的语音片段，并逐片段进行归一化，归一化公式为

$$
\begin{aligned}
& x=\left\{x_{1}, x_{2}, \ldots, x_{n}\right\}, \quad x_{i} \in \mathbb{R} \\
& x_{i}^{\text {norm }}=\frac{x_{i}}{\max _{j}\left|x_{j}\right|}, \quad \forall i=1, \ldots, n
\end{aligned}
$$

## 22. －数据增强：

- 组合方式：从噪声数据中随机抽取片段，对三类语音数据分别进行混音
- 多信噪比混音：每对片段进行 10 dB 20 dB 两种不同信噪比的混音
- 总时长：原始时长 $\times(3+1)$


## 23. 2．1．4 MFCC 特征提取

－切分：将每秒的语音片段（ 8000 个点）以每段 256 个点切分成 31 帧（尾部未满一个帧长的 64 个点被丢弃）
－MFCC：我们选用13维的MFCC以适配嵌入式对低资源占用的要求。对每帧进行MFCC，并取出第一维DC直流分量，得到每秒 $31 \times 12$ 的MFCC特征序列。

## 24. 2 模型架构与训练

## 25. 2．2．1 模型架构设计

- 模型架构与VAD对模型大致相同，但有两处修改：
- 1）将卷积核从 $3 \times 3$ 改为了 $7 \times 12$
- 增大到 7 是为了增加时间维度上的感受范围；
- 增大到 12 是为了感知频率的绝对位置，因为与MFCC的维度相等，这样在频域上没有滑动，能捕捉频率的绝对位置关系。因为识别不同说话人的时候，说话频率是个关键因素
- 2）将通道数增加到了 64
- 模型具体信息如下：
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-08.jpg?height=1197&width=275&top_left_y=123&top_left_x=935)
－总参数量： 5,635 （22．01 KB）


## 26. 2．2．2 训练与量化

## 27. －训练策略：

| 项目 | 配置 |
| :--- | :--- |
| 优化器 | Adam |
| 损失函数 | Categorical Crossentropy |
| 指标 | Accuracy |
| Epoch 数 | 20 |
| 批大小 | 32 |

－使用交叉熵损失函数训练输出为 one－hot 编码的三分类；

## 28. －模型量化：

－为部署至嵌入式设备，训练完成后进行 全整数量化（Full Integer Quantization）：

| 项目 | 配置 |
| :--- | :--- |
| 输入类型 | int8 |
| 输出类型 | int8 |
| 支持算子 | TFLITE＿BUILTINS＿INT8 |
| 校准样本 | 前 100 个训练样本 |
| 代表数据集 | 采样前 100 个样本，使用 float32 格式参与量化校准 |

－通过 representative＿dataset 提供样本校准数据，生成低精度模型，适用于资源受限环境

## 29. 3 后处理：指数平滑

－指数平滑：去掉softmax层，对全连接层输出的logits进行指数平滑，消除抖动，提高模型稳定性：

$$
\begin{aligned}
& y_{0}=x_{0} \\
& y_{t}=\alpha x_{t}+(1-\alpha) y_{t-1}, \quad t=1,2, \ldots, n, \quad \alpha \in[0,1]
\end{aligned}
$$

－考虑到大部分场景下人们说话都是一个连续的过程，说话人的切换往往伴随着一定时间的沉默，因此尝试使用指数平滑EMA算法来优化模型识别结果，即：当前识别结果以一定的权值与之前的识别结果累加，并以此类推，直到遇到非语音片段时，可能存在说话人的切换，故清空之前的识别结果并重新累加。

## 30. 3．整体推理流程仿真

## 31. 1．音频加载

## 32. 2．逐秒计算MFCC特征矩阵

## 33. 3．VAD语音活动检测：

Voice Activity Detection Results
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-09.jpg?height=311&width=1854&top_left_y=2006&top_left_x=114)
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-09.jpg?height=378&width=1876&top_left_y=2315&top_left_x=94)

4．对语音活动的区域进行说话人识别，使用模型直接推理，不使用后处理技术，识别结果如下图所示：
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-10.jpg?height=1358&width=1866&top_left_y=279&top_left_x=102)

5．对语音活动的区域进行说话人识别，使用指数平滑方法进行后处理，alpha设为 0.2 ，识别结果相比模型原始推理结果更好，如下图所示：

Speaker Diarization Results－XiaoXin vs XiaoYuan vs Others
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-10.jpg?height=665&width=1863&top_left_y=1909&top_left_x=101)
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-11.jpg?height=603&width=1869&top_left_y=98&top_left_x=98)

## 34. 4．模型部署

- MFCC权重导出：使用CMSIS库中的自动化导出脚本导出mfcc权重
- 模型权重导出：开发了模型权重自动导出脚本，可从tflite文件里自动导出适配于NMSIS－NN库的c语言权重变量和参数
－Multiplier和Shift的计算：


## 35. a．浮点系数

$$
M=\frac{s_{x} \cdot s_{w}}{s_{\mathrm{out}}}
$$

## 36. b．离线量化

- 选取 $n \in[0,31]$ 使 $2^{n} M \in[0.5,1)$
- 存 multiplier $=\operatorname{round}\left(2^{\wedge}\{\mathrm{n}\} \mathrm{M} \cdot 2^{\wedge}\{31\}\right)$（Q0．31）
- 存 shift $=\mathrm{n}$ 。
- 运行时做：（acc＊multiplier）＞＞（31＋shift），即＂multi＋shift＂。


## 37. －量化相关的其他公式：

$$
\begin{aligned}
\underbrace{\text { out }_{\text {int8 }}^{(c)}}_{\text {Q7 }}= & \operatorname{clamp}(\operatorname{round}(M^{(c)} \cdot[\underbrace{\sum_{i}\left(x_{i}-z p_{x}\right) w_{i}^{(c)}}_{\text {int32 累加 }}+b_{\text {int32 }}^{(c)}])+z p_{\text {out }}) \\
& \sum\left(x_{\text {int }}-z p_{x}\right) w_{\text {int }}+b_{\text {int }}=\frac{\sum x^{\mathrm{fp}} w^{\mathrm{fp}}}{s_{x} s_{w}}+\frac{b^{\mathrm{fp}}}{s_{x} s_{w}}
\end{aligned}
$$

－PC与QEMU的对齐：第一次将模型导出并成功部署到QEMU上运行时，因为使用了NMSIS－NN库提供的API，所以不清楚具体的API如riscv＿convolve＿wrapper＿s8在卷积时对边界补0的策略，以及全连接层的一些策略是否与PC推理一致；发现QEMU的推理结果与PC上的推理结果不一致，尝试了很多办法，最后将模型一层一层的输出分别与QEMU和PC进行比较，成功将卷积和池化层的输出在PC和QEMU上对齐了，但全连接层始终无法对齐；最后手动实现了一个全连接函数，才实现模型推理结果在PC和QEMU上完全对齐。

## 38. 三、软件工作流程

系统上电并完成初始化后，创建speech＿task用来获取和保存虚拟PCM数据，创建algo＿task用来处理PCM数据，实现人声检测和说话人识别，程序运行流程图如下图所示：
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-12.jpg?height=988&width=1330&top_left_y=410&top_left_x=370)
－首先创建algo＿task，在algo＿task中完成event manager的创建和注册；随后调用NMSIS DSP库中的riscv＿mfcc＿init＿256＿f32（）函数实现mfcc初始化；随后调用malloc分配mfcc相关计算缓冲区；最后在while（1）中调用vpi＿event＿listen函数开始监听ID为EVENT＿SEN＿DATA＿READY的消息；
－然后创建speech＿task，在speech＿task中调用hal＿pdm＿get＿device（）获取PDM的实例句柄，使用hal＿pdm＿init（）函数进行PDM初始化，设置缓冲区pdm＿buf和回调函数pdm＿irq＿handler（），然后调用hal＿pdm＿start（）函数启动PDM；
－为了实现高效且安全的PCM数据内存管理，使用了环形缓冲区RingBuffer来存储pdm＿buf的数据，RingBuffer由静态分配的数组和一个读索引、一个写索引组成，长度设置为32001byte，可以存储2秒的PCM数据，且当RingBuffer数据存满时，会自动覆盖最前面的数据，避免数据溢出；
－当pdm＿buf收满数据后触发PDM中断，自动调用pdm＿irq＿handler（），在中断回调函数中，将 pdm＿buf中的数据写入RingBuffer，并对RingBuffer中存储的数据数量进行判断，如果RingBuffer已存储了超过 16000 byte（即 1 秒）的数据，向algo＿task发送EVENT，通知algo＿task开始处理数据；
－algo＿task收到EVENT后，调用event＿handler函数，在该函数中完成算法的处理；先从RingBuffer中取出16000byte（1秒）的数据到自己的临时缓冲区；然后将该16000个uint8＿t格式的数据转换为8000个float32＿t的语音PCM数据，并调用NMSIS DSP库中的riscv＿mfcc＿f32（）函数进行mfcc运算，结果保存为二维数组mfcc＿results［31］［13］（为了尽可能多的保存mfcc特征值精度，使用了 f 32 格式进行运算）；
－mfcc完成后进行VAD人声检测，先将float32＿t格式的mfcc特征数组量化为满足VAD模型的int8＿t格式；然后进行VAD模型推理，包括卷积、池化和全连接，根据全连接的输出值实现人声检测；若检测到不是人声，则打印结果并结束推理；如果检测到是人声，则进行SPK说话人识别；
－spk说话人识别同样先将float32＿t格式的mfcc特征数组量化为满足SPK模型的int8＿t格式；然后进行SPK模型推理，包括卷积、池化和全连接，并根据全连接的输出值实现说话人识别，并打印识别结果；
－算法已处理音频量统计：定义一个初始值为0的全局变量以存储算法已处理音频量，每次algo＿task从RingBuffer中取数据时，将取出的数据量累加到该全局变量中，并在打印识别结果时将该全局变量根据采样率转换为时间显示在打印结果中。

## 39. 四、验证及测试结果

－将源代码编译并运行，QEMU开始打印识别结果，如下图所示：

```
日 Console x : Problems (Dxecutables 穓Debugger Console
<terminated> debug_qemu [GDB Nuclei QEMU riscv Debugging] qemu
GDB Server listening on: 'tcp::1234'...
soc init done
Board: Qemu
Hello VeriHealthi!
algo manager create succeed
algo manager register succeed
mfcc compute module init done
pdm init succeed
pdm start succeed
qemu-system-riscv32.exe: warning: GLib-GIO: Unexpectedly,
1000 ms, 无人声
2000 ms, 无人声
3000 ms, 无人声
4000 ms, 无人声
5 0 0 0 \mathrm { ms } \text { , 说话人ID: 其他说话人 }
6000 ms, 说话人ID: 其他说话人
7 0 0 0 \mathrm { ms } \text { , 说话人ID: 其他说话人 }
8000 ms,无人声
```

－QEMU对其它说话人识别结果准确率很高，并且能将说话人中间的暂时停顿识别为无人声

```
2 0 0 0 0 \mathrm { ms } \text { ,无人声}
2 1 0 0 0 \mathrm { ms } \text { ,无人声}
2 2 0 0 0 \mathrm { ms } \text { , 说诂人ID:具他说诂人 }
2 3 0 0 0 \mathrm { ms } \text { , 说话人ID: 其他说话人 }
2 4 0 0 0 \mathrm { ms } \text { , 说话人ID: 其他说话人 }
2 5 0 0 0 \mathrm { ms } \text { , 说话人ID: 其他说话人 }
2 6 0 0 0 \mathrm { ms } \text { , 说话人ID: 其他说话人 }
2 7 0 0 0 \mathrm { ms } \text { , 说话人ID: 其他说话人 }
2 8 0 0 0 \mathrm { ms } \text { , 说话人ID: 其他说话人 }
2 9 0 0 0 \mathrm { ms } \text { , 说话人ID: 其他说话人 }
30000 ms, 说话人ID: 其他说话人
31000 ms, 说话人ID: 其他说话人
32000 ms, 说话人ID: 其他说话人
3 3 0 0 0 \mathrm { ms, } \mathrm { 九人戸 }
```

![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-13.jpg?height=47&width=300&top_left_y=2597&top_left_x=181)

```
其它说话人识别结果
```

3000 ms, 无人声
4000 ms, 无人声
5000 ms, 说话人ID：其他说话人
6000 ms, 说话人ID：其他说话人
7000 ms, 说话人ID：其他说话人
8000 ms, 无人声
9000 ms, 说话人ID：其他说话人
10000 ms, 说话人ID：其他说话人
11000 ms, 说话人ID：其他说话人
12000 ms, 说话人ID：其他说话人
13000 ms, 说话人ID：其他说话人
14000 ms, 说话人ID：其他说话人
15000 ms, 说话人ID：其他说话人
16000 ms, 说话人ID：其他说话人
17000 ms, 说话人ID：其他说话人
18000 ms, 说话人ID：其他说话人
19000 ms, 无人声
20000 ms, 无人声

其它说话人识别结果
－QEMU对小原识别结果准确率较低，容易把小原识别为其他说话人；但对小芯识别准确率很高，在 VAD提供的语音片段上实现了 $100 \%$ 的识别准确率
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-14.jpg?height=597&width=529&top_left_y=352&top_left_x=283)

小原识别结果
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-14.jpg?height=566&width=491&top_left_y=353&top_left_x=1226)

小芯识别结果
－QEMU对结尾的音乐片段识别准确率很高，全都正确识别为了无人声
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-14.jpg?height=694&width=462&top_left_y=1159&top_left_x=787)
－在QEMU上最终识别结果总览：
![](https://cdn.mathpix.com/cropped/2025_07_23_5383e27041ad485e47a4g-14.jpg?height=643&width=964&top_left_y=1988&top_left_x=556)

QEMU识别结果总览
－总结：成功在QEMU虚拟机上运行了语音活动检测和说话人识别算法。语音活动检测算法对人声的检测效果很好，能全部过滤掉末尾的音乐片段，且能识别说话人中间的暂时停顿。说话热识别算法对其他说话人的识别准确率为 $100 \%$（对于所有其他说话人样本，都正确识别为了其他说话人），对小芯识别准确率为 $100 \%$ ，对小原识别率稍差， 11 个点中识别出了 6 个点，其余 5 个点识别成了其它说话人。在小原的识别准确率上仍有待优化。

## 40. 五、注意事项

1．工程文件使用了RISCV＿DSP静态库与RISCV＿NN静态库，均为从github上Nuclei－Software／NMSIS克隆并编译而来，编译工程时必须添加这两个库。

2．应用代码均位于工程根目录下的app子目录中。

## 41. 六、团队介绍及分工

| 姓名 | 单位 | 年级 | 分工 |
| :--- | :--- | :--- | :--- |
| 舒星研（队长） | 电子科技大学自动化工程学院 | 研二 | 嵌入式软件开发、模型部署 |
| 李东昊 | 电子科技大学自动化工程学院 | 研二 | 算法设计、模型训练、量化和部署 |
| 张含 | 电子科技大学自动化工程学院 | 研二 | BLE蓝牙协议栈开发 |

